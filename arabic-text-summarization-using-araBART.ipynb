{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2950550,"sourceType":"datasetVersion","datasetId":1808997},{"sourceId":2982635,"sourceType":"datasetVersion","datasetId":1828076},{"sourceId":6873441,"sourceType":"datasetVersion","datasetId":3949723}],"dockerImageVersionId":30558,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Transformers installation\n! pip install torch -q\n! pip install transformers[torch] datasets -q\n# To install from source instead of the last release, comment the command above and uncomment the following one.\n! pip install git+https://github.com/huggingface/transformers.git","metadata":{"id":"REKkkSbJ3Tdr","outputId":"02219334-95da-4d35-aba1-35c60507cb8b","execution":{"iopub.status.busy":"2023-11-22T07:05:36.64596Z","iopub.execute_input":"2023-11-22T07:05:36.646504Z","iopub.status.idle":"2023-11-22T07:06:46.647497Z","shell.execute_reply.started":"2023-11-22T07:05:36.646476Z","shell.execute_reply":"2023-11-22T07:06:46.646465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\nimport os\nwith open(\"/kaggle/input/wikilingua-arabic-summarisation/arabic.pkl\", 'rb') as pickle_file:\n    arabic_docs=pickle.load(pickle_file)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:06:50.084127Z","iopub.execute_input":"2023-11-22T07:06:50.084937Z","iopub.status.idle":"2023-11-22T07:06:52.045949Z","shell.execute_reply.started":"2023-11-22T07:06:50.084896Z","shell.execute_reply":"2023-11-22T07:06:52.045095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\n\n# Flatten the nested dictionary into a list of dictionaries\nflat_data = []\nfor url, sections in arabic_docs.items():\n    for section_name, section_data in sections.items():\n        flat_data.append({\n            'input_text': section_data['document'],\n            'target_text': section_data['summary'],\n        })\n\n# Create a DataFrame from the flattened data\ndf = pd.DataFrame(flat_data)\n\n# Create a dataset from the DataFrame\nsummarization_dataset = Dataset.from_pandas(df)\n\n# Display the first few rows of the dataset\nsummarization_dataset\n","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:06:55.417209Z","iopub.execute_input":"2023-11-22T07:06:55.417594Z","iopub.status.idle":"2023-11-22T07:06:57.514803Z","shell.execute_reply.started":"2023-11-22T07:06:55.417561Z","shell.execute_reply":"2023-11-22T07:06:57.513911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(list(arabic_docs.items())[0])\nlist(arabic_docs.items())[0]","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:06:59.879773Z","iopub.execute_input":"2023-11-22T07:06:59.880316Z","iopub.status.idle":"2023-11-22T07:06:59.966977Z","shell.execute_reply.started":"2023-11-22T07:06:59.880261Z","shell.execute_reply":"2023-11-22T07:06:59.966069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summarization_dataset.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:07:04.843106Z","iopub.execute_input":"2023-11-22T07:07:04.843885Z","iopub.status.idle":"2023-11-22T07:07:04.849689Z","shell.execute_reply.started":"2023-11-22T07:07:04.843848Z","shell.execute_reply":"2023-11-22T07:07:04.848799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Summarization","metadata":{"id":"XyDAw1MW3Tdw"}},{"cell_type":"markdown","source":"Summarization creates a shorter version of a document or an article that captures all the important information. Along with translation, it is another example of a task that can be formulated as a sequence-to-sequence task. \n\n<!--This tip is automatically generated by `make fix-copies`, do not fill manually!-->\n\n[BART](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/bart), [BigBird-Pegasus](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/bigbird_pegasus), [Blenderbot](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/blenderbot), [BlenderbotSmall](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/blenderbot-small), [Encoder decoder](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/encoder-decoder), [FairSeq Machine-Translation](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/fsmt), [GPTSAN-japanese](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/gptsan-japanese), [LED](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/led), [LongT5](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/longt5), [M2M100](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/m2m_100), [Marian](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/marian), [mBART](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mbart), [MT5](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mt5), [MVP](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mvp), [NLLB](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/nllb), [NLLB-MOE](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/nllb-moe), [Pegasus](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/pegasus), [PEGASUS-X](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/pegasus_x), [PLBart](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/plbart), [ProphetNet](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/prophetnet), [SwitchTransformers](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/switch_transformers), [T5](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/t5), [XLM-ProphetNet](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/xlm-prophetnet)\n\n<!--End of the generated tip-->\n\n</Tip>\n\nBefore you begin, make sure you have all the necessary libraries installed:\n\n```bash\npip install transformers datasets evaluate rouge_score\n```\n\nWe encourage you to login to your Hugging Face account so you can upload and share your model with the community. When prompted, enter your token to login:","metadata":{"id":"N3oVn22t3Td0"}},{"cell_type":"code","source":"! pip install nltk rouge_score","metadata":{"id":"cGE0nFjP47G3","outputId":"4a2ec9cc-6d96-46cd-e95f-6a49a59904db","execution":{"iopub.status.busy":"2023-11-22T07:07:11.632958Z","iopub.execute_input":"2023-11-22T07:07:11.633299Z","iopub.status.idle":"2023-11-22T07:07:25.456058Z","shell.execute_reply.started":"2023-11-22T07:07:11.63326Z","shell.execute_reply":"2023-11-22T07:07:25.454859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load arabic_docs dict","metadata":{"id":"mNhRPZix3Td2"}},{"cell_type":"markdown","source":"Split the dataset into a train and test set with the [train_test_split](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.train_test_split) method:","metadata":{"id":"eL1GACgq3Td5"}},{"cell_type":"code","source":"# split your arabic dataset here \nfrom sklearn.model_selection import train_test_split\n\nsummarization_dataset = summarization_dataset.train_test_split(test_size=0.2)\nprint(\"done!\")","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:07:28.84446Z","iopub.execute_input":"2023-11-22T07:07:28.845252Z","iopub.status.idle":"2023-11-22T07:07:29.339951Z","shell.execute_reply.started":"2023-11-22T07:07:28.845213Z","shell.execute_reply":"2023-11-22T07:07:29.338965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summarization_dataset['train']","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:07:32.230254Z","iopub.execute_input":"2023-11-22T07:07:32.230629Z","iopub.status.idle":"2023-11-22T07:07:32.237142Z","shell.execute_reply.started":"2023-11-22T07:07:32.230599Z","shell.execute_reply":"2023-11-22T07:07:32.236236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocess","metadata":{"id":"mUC74n6y3Td7"}},{"cell_type":"markdown","source":"in our model we used araBERT","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade transformers huggingface-hub","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:07:37.558801Z","iopub.execute_input":"2023-11-22T07:07:37.559627Z","iopub.status.idle":"2023-11-22T07:07:49.282697Z","shell.execute_reply.started":"2023-11-22T07:07:37.559591Z","shell.execute_reply":"2023-11-22T07:07:49.281663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq, BarthezTokenizer, MBartForConditionalGeneration\n\ntokenizer = BarthezTokenizer.from_pretrained(\"moussaKam/AraBART\")\nmodel = MBartForConditionalGeneration.from_pretrained(\"moussaKam/AraBART\")\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:19:08.949867Z","iopub.execute_input":"2023-11-22T07:19:08.950652Z","iopub.status.idle":"2023-11-22T07:19:12.383436Z","shell.execute_reply.started":"2023-11-22T07:19:08.950614Z","shell.execute_reply":"2023-11-22T07:19:12.382639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_summary(text):\n    # Tokenize the input text\n    input_ids = tokenizer.encode(text, return_tensors='pt')\n\n    # Generate the summary\n    summary_ids = model.generate(input_ids, num_beams=4, max_length=100, early_stopping=True, forced_bos_token_id=tokenizer.lang_code_to_id[\"ar_AR\"])\n    \n    # Decode the generated summary\n    summary = tokenizer.decode(summary_ids.squeeze(), skip_special_tokens=True)\n    \n    return summary\n\nprint('done !')","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:19:17.161954Z","iopub.execute_input":"2023-11-22T07:19:17.162329Z","iopub.status.idle":"2023-11-22T07:19:17.168452Z","shell.execute_reply.started":"2023-11-22T07:19:17.16229Z","shell.execute_reply":"2023-11-22T07:19:17.167535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_input_length = 1024\nmax_target_length = 128\n\ndef preprocess_function(examples):\n    model_inputs = tokenizer(examples[\"input_text\"], max_length=max_input_length, truncation=True)\n\n    # Setup the tokenizer for targets\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(examples[\"target_text\"], max_length=max_target_length, truncation=True)\n    \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\nprint('done !')","metadata":{"id":"HOh0ovGK3Td8","execution":{"iopub.status.busy":"2023-11-22T07:19:20.538146Z","iopub.execute_input":"2023-11-22T07:19:20.538518Z","iopub.status.idle":"2023-11-22T07:19:20.545134Z","shell.execute_reply.started":"2023-11-22T07:19:20.538487Z","shell.execute_reply":"2023-11-22T07:19:20.544055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To apply the preprocessing function over the entire dataset, use 🤗 Datasets [map](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map) method. You can speed up the `map` function by setting `batched=True` to process multiple elements of the dataset at once:","metadata":{"id":"tqEEXQgW3Td8"}},{"cell_type":"code","source":"tokenized_datasets = summarization_dataset.map(preprocess_function, batched=True)","metadata":{"id":"rWwzePu43Td8","outputId":"4775d6c1-f05f-440a-ad7e-b92ad78c018f","execution":{"iopub.status.busy":"2023-11-22T07:19:24.216411Z","iopub.execute_input":"2023-11-22T07:19:24.216782Z","iopub.status.idle":"2023-11-22T07:21:08.524686Z","shell.execute_reply.started":"2023-11-22T07:19:24.216753Z","shell.execute_reply":"2023-11-22T07:21:08.523796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_datasets","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:21:26.00443Z","iopub.execute_input":"2023-11-22T07:21:26.00518Z","iopub.status.idle":"2023-11-22T07:21:26.011035Z","shell.execute_reply.started":"2023-11-22T07:21:26.005148Z","shell.execute_reply":"2023-11-22T07:21:26.010139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate","metadata":{"id":"V9a3IMGW3Td9"}},{"cell_type":"code","source":"from datasets import load_metric\n\nrouge = load_metric(\"rouge\")","metadata":{"id":"Seg29MAO3Td9","outputId":"2a4235df-2d4e-4c5d-e055-bd9a352738cb","execution":{"iopub.status.busy":"2023-11-22T07:09:24.434214Z","iopub.execute_input":"2023-11-22T07:09:24.434607Z","iopub.status.idle":"2023-11-22T07:09:25.479841Z","shell.execute_reply.started":"2023-11-22T07:09:24.434571Z","shell.execute_reply":"2023-11-22T07:09:25.479039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n\n    return {k: round(v, 4) for k, v in result.items()}\n\nprint('done !')","metadata":{"id":"nZsNGOzc3Td-","execution":{"iopub.status.busy":"2023-11-22T07:21:40.011451Z","iopub.execute_input":"2023-11-22T07:21:40.011825Z","iopub.status.idle":"2023-11-22T07:21:40.020033Z","shell.execute_reply.started":"2023-11-22T07:21:40.011795Z","shell.execute_reply":"2023-11-22T07:21:40.018901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train","metadata":{"id":"Db5-IU5w3Td-"}},{"cell_type":"code","source":"training_args = Seq2SeqTrainingArguments(\n    output_dir=\"my_model\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    weight_decay=0.01,\n    save_total_limit=3,\n    num_train_epochs=4,\n    predict_with_generate=True,\n    push_to_hub=False,\n    report_to=\"tensorboard\",  # Set this to \"none\" if you want to disable all integrations\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=summarization_dataset['train'],\n    eval_dataset=summarization_dataset['test'],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)","metadata":{"id":"qAPhdpwO3Td_","execution":{"iopub.status.busy":"2023-11-22T07:22:42.594655Z","iopub.execute_input":"2023-11-22T07:22:42.595371Z","iopub.status.idle":"2023-11-22T07:22:42.761134Z","shell.execute_reply.started":"2023-11-22T07:22:42.59534Z","shell.execute_reply":"2023-11-22T07:22:42.760335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hugginface use wandab to log training \n# لن نحتاج إليه لذلك سوف نقوم بتعطيله\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:22:46.890625Z","iopub.execute_input":"2023-11-22T07:22:46.891003Z","iopub.status.idle":"2023-11-22T07:22:46.895649Z","shell.execute_reply.started":"2023-11-22T07:22:46.890973Z","shell.execute_reply":"2023-11-22T07:22:46.894736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train","metadata":{"id":"fxUw6kUq55jN","outputId":"b673933f-817f-4864-dda5-cb8c27362d17","execution":{"iopub.status.busy":"2023-11-22T07:22:48.007995Z","iopub.execute_input":"2023-11-22T07:22:48.008844Z","iopub.status.idle":"2023-11-22T07:22:48.014615Z","shell.execute_reply.started":"2023-11-22T07:22:48.008807Z","shell.execute_reply":"2023-11-22T07:22:48.013566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#trainer.push_to_hub()\ntrainer.save_model('./mymodel')","metadata":{"id":"7ZGzmTLO3TeA","outputId":"f4be15cb-92e1-41db-809e-08b1a746bc6c","execution":{"iopub.status.busy":"2023-11-22T07:22:50.579094Z","iopub.execute_input":"2023-11-22T07:22:50.580038Z","iopub.status.idle":"2023-11-22T07:22:51.675855Z","shell.execute_reply.started":"2023-11-22T07:22:50.580001Z","shell.execute_reply":"2023-11-22T07:22:51.674763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{"id":"KUbi7XMU3TeA"}},{"cell_type":"code","source":"text = \"الرياض هي عاصمة المملكة العربية السعودية وأكبر مدينة فيها. تعد الرياض مركزاً حضرياً حديثاً توفر العديد من المرافق والخدمات. تشتهر المدينة بأبراجها الحديثة ومراكز التسوق الفاخرة. يمكن للزوار استكشاف التاريخ الغني للمنطقة من خلال زيارة المتاحف والمعالم الثقافية. الرياض تجمع بين التقاليد العربية والحياة الحديثة، مما يجعلها وجهة مثيرة ومتنوعة للسياح.\"","metadata":{"id":"s1HZW44n3TeA","execution":{"iopub.status.busy":"2023-11-22T07:37:09.449471Z","iopub.execute_input":"2023-11-22T07:37:09.449912Z","iopub.status.idle":"2023-11-22T07:37:09.454775Z","shell.execute_reply.started":"2023-11-22T07:37:09.44988Z","shell.execute_reply":"2023-11-22T07:37:09.453737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"this model was made using araBERT. apparently the pipeline does not support this model.\nthis model will be bilt again durring this week using mt5. \ni spent much time building it, and i have lots of other things to do so i will be sharing this version of the TP and will apdate you with the mt5 version as soon as possible this week.\n\ni hope you'll understand.\n","metadata":{}},{"cell_type":"code","source":"import torch\n\n# Assuming 'text' is your input data\ninput_ids = tokenizer.encode(text, return_tensors=\"pt\")\n\n# Move input to the same device as the model\ninput_ids = input_ids.to(model.device)\nprint('done !')","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:37:12.120076Z","iopub.execute_input":"2023-11-22T07:37:12.120465Z","iopub.status.idle":"2023-11-22T07:37:12.12786Z","shell.execute_reply.started":"2023-11-22T07:37:12.120431Z","shell.execute_reply":"2023-11-22T07:37:12.12693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:37:14.289236Z","iopub.execute_input":"2023-11-22T07:37:14.289972Z","iopub.status.idle":"2023-11-22T07:37:14.295405Z","shell.execute_reply.started":"2023-11-22T07:37:14.289938Z","shell.execute_reply":"2023-11-22T07:37:14.29451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate summary\nsummary_ids = model.generate(input_ids)\nsummary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\nprint('The orginal text :', text)\nprint('\\n')\nprint(\"Generated Summary:\", summary)\nprint('\\n \\n This summary was made using the abstractive summarization (text generation).')","metadata":{"execution":{"iopub.status.busy":"2023-11-22T07:39:08.894054Z","iopub.execute_input":"2023-11-22T07:39:08.894509Z","iopub.status.idle":"2023-11-22T07:39:09.098039Z","shell.execute_reply.started":"2023-11-22T07:39:08.894473Z","shell.execute_reply":"2023-11-22T07:39:09.097137Z"},"trusted":true},"execution_count":null,"outputs":[]}]}